# MLPapers

A list of all ML papers I want to read, and notes I've taken on each paper. Will be updated over time (probably quite slowly lol).

**Generic Papers I find interesting and eventually want to read**

| Paper Name/Link                                                                                                                           | Read? | Notes |
| ----------------------------------------------------------------------------------------------------------------------------------------- | ----- | ----- |
| [Adam: A method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)                                                                |       |       |
| [The Curse of Dimensionality](https://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/dimensionality.pdf)                     |       |       |
| [The Noble Eightfold Path to Linear Regression](https://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/LinearRegression.pdf) |       |       |
| [Logistic Regression as Soft Perceptron Learning]()                                                                                          |       |       |
| [Why the Normal Distribution?](https://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/Gaussian-distribution.pdf)             |       |       |
| [The Secret Life of the Covariance Matrix](https://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/secretcovariance.pdf)      |       |       |

**Classic ML Papers**

| Paper Name/Link                                                                                                                                                                                       | Read? | Notes                                  |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | -------------------------------------- |
| [Regression and Shrinkage via the Lasso](https://www.jstor.org/stable/2346178)                                                                                                                           | ✅    | [here](Classic%20%ML%20%Papers/Lasso.tex) |
| [Regularization and variable selection via the elastic net](https://doi.org/10.1111/j.1467-9868.2005.00503.x)                                                                                            |       |                                        |
| [Classification and Regression Trees](https://pages.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf)                                                                                                      |       |                                        |
| [Random Forests](https://link.springer.com/article/10.1023/A:1010933404324)                                                                                                                              |       |                                        |
| [Adaboost Tutorial](https://www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf)                                                                                                                               | ✅    |                                        |
| [Bagging Predictors](https://link.springer.com/article/10.1007/BF00058655)                                                                                                                               |       |                                        |
| [A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting (AdaBoost)](https://www.sciencedirect.com/science/article/pii/S002200009791504X)                                 |       |                                        |
| [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754)                                                                                                                             |       |                                        |
| [Bias Plus Variance Decomposition for Zero-One Loss Functions](https://www.researchgate.net/publication/2793430_Bias_Plus_Variance_Decomposition_for_Zero-One_Loss_Functions)                            |       |                                        |
| [Bootstrap Methods: Another Look at the Jackknife](https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full) |       |                                        |
| [Training Algorithm for Optimal Margin Classifier](https://dl.acm.org/doi/10.1145/130385.130401)                                                                                                         |       |                                        |
| [SMOTE: Synthetic Minority Over-sampling Technique](https://www.jair.org/index.php/jair/article/view/10302/24590)                                                                                        |       |                                        |
| [Nearest Neighbor Pattern Classification](https://ieeexplore.ieee.org/document/1053964)                                                                                                                  |       |                                        |
| [The Strength of Weak Learnability](http://rob.schapire.net/papers/strengthofweak.pdf)                                                                                                                   |       |                                        |

# **DL Papers**

I probably won't give notes for most of these, but the replications will be in DL Papers

| Paper Name/Link | Read? | Notes / Replications |
| --------------- | ----- | -------------------- |
|                 |       |                      |
|                 |       |                      |
|                 |       |                      |
